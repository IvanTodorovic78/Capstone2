{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2 Project\n",
    "\n",
    "## NLP training on ca. 3 million Yelp reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is covered in this part of the project\n",
    "This part of the project deals with natural language processing of the Yelp data set. Specifically, it will import the raw data in json format, filter the reviews that relate to restaurants, prepare the data for NLP and perform a number of NLP functions:\n",
    "\n",
    "1. Text preparation, including tokenization, lemmatization and normalization of the text using the spaCy library\n",
    "1. Phrase modeling using gensim\n",
    "1. Topic modeling with LDA\n",
    "1. Word vector models with word2vec\n",
    "1. Clustering words that commonly appear together \n",
    "\n",
    "Topic modeling and word2vec clusters will provide context to the reviews in order to return summary information that is easily interpretable and understandable to an average user, and may provide useful insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Yelp data\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!\n",
    "\n",
    "**Note:** If you'd like to execute this notebook interactively on your local machine, you'll need to download your own copy of the Yelp dataset. \n",
    "\n",
    "After filtering for restaurants, there are approximately __52K__ restaurants with approximately __2.9M__ user reviews related to them.\n",
    "\n",
    "The raw data is available in six of files in _.json_ format, of which two are relevant for the project:\n",
    "- __business.json__ &mdash; _the records for individual businesses_\n",
    "- __review.json__ &mdash; _the records for reviews users wrote about businesses_\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\": \"YDf95gJZaq05wvo7hTQbbQ\", \"name\": \"Richmond Town Square\", \"neighborhood\": \"\", \"address\": \"691 Richmond Rd\", \"city\": \"Richmond Heights\", \"state\": \"OH\", \"postal_code\": \"44143\", \"latitude\": 41.5417162, \"longitude\": -81.4931165, \"stars\": 2.0, \"review_count\": 17, \"is_open\": 1, \"attributes\": {\"RestaurantsPriceRange2\": 2, \"BusinessParking\": {\"garage\": false, \"street\": false, \"validated\": false, \"lot\": true, \"valet\": false}, \"BikeParking\": true, \"WheelchairAccessible\": true}, \"categories\": [\"Shopping\", \"Shopping Centers\"], \"hours\": {\"Monday\": \"10:00-21:00\", \"Tuesday\": \"10:00-21:00\", \"Friday\": \"10:00-21:00\", \"Wednesday\": \"10:00-21:00\", \"Thursday\": \"10:00-21:00\", \"Sunday\": \"11:00-18:00\", \"Saturday\": \"10:00-21:00\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "data_directory = os.path.join('data',\n",
    "                              'dataset')\n",
    "\n",
    "businesses_filepath = os.path.join(data_directory,\n",
    "                                   'business.json')\n",
    "\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print first_business_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business records consist of _key, value_ pairs containing information about the particular business. The information from this file is treated in a separate notebook named _\"Restaurants\"_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will process information contained in the _reviews.json_ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"VfBHSwC5Vz_pbFluy07i9Q\",\"user_id\":\"cjpdDjZyprfyDG3RlkVG3w\",\"business_id\":\"uYHaNptLzDLoV_JZ_MuzUA\",\"stars\":5,\"date\":\"2016-07-12\",\"text\":\"My girlfriend and I stayed here for 3 nights and loved it. The location of this hotel and very decent price makes this an amazing deal. When you walk out the front door Scott Monument and Princes street are right in front of you, Edinburgh Castle and the Royal Mile is a 2 minute walk via a close right around the corner, and there are so many hidden gems nearby including Calton Hill and the newly opened Arches that made this location incredible.\\n\\nThe hotel itself was also very nice with a reasonably priced bar, very considerate staff, and small but comfortable rooms with excellent bathrooms and showers. Only two minor complaints are no telephones in room for room service (not a huge deal for us) and no AC in the room, but they have huge windows which can be fully opened. The staff were incredible though, letting us borrow umbrellas for the rain, giving us maps and directions, and also when we had lost our only UK adapter for charging our phones gave us a very fancy one for free.\\n\\nI would highly recommend this hotel to friends, and when I return to Edinburgh (which I most definitely will) I will be staying here without any hesitation.\",\"useful\":0,\"funny\":0,\"cool\":0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory,\n",
    "                                    'review.json')\n",
    "\n",
    "with codecs.open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print first_review_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few attributes of note on the review records:\n",
    "- __business\\_id__ &mdash; _identifies the business in question and provides a link to each business in the business.json file_\n",
    "- __text__ &mdash; _the actual text of the review written by the user_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work required to prepare the text for NLP analysis requires the following:\n",
    "1. Loading each business record line by line using _json.loads_. Python converts json objects into a Python dict\n",
    "2. Filter each business record to include only those that have \"Restaurant: in the category list\n",
    "3. Form a set of the business_id records in order to use this filter when processing the reviews file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,613 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        if u'Restaurants' not in business[u'categories']:\n",
    "            continue\n",
    "            \n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        restaurant_ids.add(business[u'business_id'])\n",
    "\n",
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print '{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step creates a new file that contains only the text from reviews about restaurants, with one review per line in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('data', 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory,\n",
    "                                   'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 2,929,512 restaurant reviews in the txt file.\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this part is complete so make the statement false to skip directly to reading the file\n",
    "if 0 == 1:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with codecs.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        # open the existing review json file\n",
    "        with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "            # loop through all reviews in the existing file and convert to dict\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not about a restaurant, skip to the next one\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print u'''Text from {:,} restaurant reviews\n",
    "              written to the new txt file.'''.format(review_count)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print u'Text from {:,} restaurant reviews in the txt file.'.format(review_count + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preparation using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**spaCy**](https://spacy.io) library will be used to perform the following NLP tasks: \n",
    "- Tokenization\n",
    "- Text normalization, including converting text to lemmatized form \n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "spaCy provides downloadable English-language models that enable the comparison of a specific corpus of text to a general English language corpus, such as common stpowords, and the probability of occurence of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is needed to append the system path list to access the location of the spaCy module and English language model on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\Windows\\System32\\.env\\Lib\\site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part now handles the text processing using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "# English model will only load with this code, does not work using the code provided in spaCy documentation\n",
    "nlp = en_core_web_sm.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The staff here is great and they're nice,  wonderful and quick. People were ranting in raving about pei wei, I had to try it.  Even good yelp reviews.  I'm highly dissatisfied with the flavor of the food. This  should be labeled Asian inspired and not Asian. I've tried a variety of Chinese restaurants, this doesn't taste close to anything I've had at other Asian restaurants. Their Mongolian beef  was 5 pieces of beef and large mushrooms cut into thirds in a thick sauce. You eat the rice to wash off the nasty flavor. My shrimp was thickly coated in an overpowering  sauce as well.  I only ate some of the veggies that take center stage on a meat dish.  The center of my pork egg roll was cold. The hot N sour soup was a much thicker consistency almost like that of a chili instead of being brothy. Worst of all was the price.  This was not worth it to us. Neither me or my husband enjoyed either of  our dishes.  We didn't even eat half of our plates.  We even refused to take it home with us.  If you like and enjoy what typical Asian food tastes like,  don't waste your time here.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this examines a sample review\n",
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print sample_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence detection and segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "The staff here is great and they're nice,  wonderful and quick.\n",
      "\n",
      "Sentence 2:\n",
      "People were ranting in raving about pei wei, I had to try it.  \n",
      "\n",
      "Sentence 3:\n",
      "Even good yelp reviews.  \n",
      "\n",
      "Sentence 4:\n",
      "I'm highly dissatisfied with the flavor of the food.\n",
      "\n",
      "Sentence 5:\n",
      "This  should be labeled Asian inspired and not Asian.\n",
      "\n",
      "Sentence 6:\n",
      "I've tried a variety of Chinese restaurants, this doesn't taste close to anything I've had at other Asian restaurants.\n",
      "\n",
      "Sentence 7:\n",
      "Their Mongolian beef  was 5 pieces of beef and large mushrooms cut into thirds in a thick sauce.\n",
      "\n",
      "Sentence 8:\n",
      "You eat the rice to wash off the nasty flavor.\n",
      "\n",
      "Sentence 9:\n",
      "My shrimp was thickly coated in an overpowering  sauce as well.  \n",
      "\n",
      "Sentence 10:\n",
      "I only ate some of the veggies that take center stage on a meat dish.  \n",
      "\n",
      "Sentence 11:\n",
      "The center of my pork egg roll was cold.\n",
      "\n",
      "Sentence 12:\n",
      "The hot N sour soup was a much thicker consistency almost like that of a chili instead of being brothy.\n",
      "\n",
      "Sentence 13:\n",
      "Worst of all was the price.  \n",
      "\n",
      "Sentence 14:\n",
      "This was not worth it to us.\n",
      "\n",
      "Sentence 15:\n",
      "Neither me or my husband enjoyed either of  our dishes.  \n",
      "\n",
      "Sentence 16:\n",
      "We didn't even eat half of our plates.  \n",
      "\n",
      "Sentence 17:\n",
      "We even refused to take it home with us.  \n",
      "\n",
      "Sentence 18:\n",
      "If you like and enjoy what typical Asian food tastes like,  don't waste your time here.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_review = nlp(sample_review)\n",
    "\n",
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print 'Sentence {}:'.format(num + 1)\n",
    "    print sentence\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named entity detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Asian - NORP\n",
      "\n",
      "Entity 2: Asian - NORP\n",
      "\n",
      "Entity 3: Chinese - NORP\n",
      "\n",
      "Entity 4: Asian - NORP\n",
      "\n",
      "Entity 5: Mongolian - NORP\n",
      "\n",
      "Entity 6: 5 - CARDINAL\n",
      "\n",
      "Entity 7: half - CARDINAL\n",
      "\n",
      "Entity 8: Asian - NORP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print 'Entity {}:'.format(num + 1), entity, '-', entity.label_\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of speech tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'re</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quick</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>People</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>were</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranting</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raving</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>about</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pei</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wei</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>had</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>try</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>refused</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>take</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>home</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>us</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>If</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>you</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>like</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>what</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>typical</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Asian</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>food</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tastes</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>like</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>do</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>n't</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>waste</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>your</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>time</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>here</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text part_of_speech\n",
       "0          The            DET\n",
       "1        staff           NOUN\n",
       "2         here            ADV\n",
       "3           is           VERB\n",
       "4        great            ADJ\n",
       "5          and          CCONJ\n",
       "6         they           PRON\n",
       "7          're           VERB\n",
       "8         nice            ADJ\n",
       "9            ,          PUNCT\n",
       "10                      SPACE\n",
       "11   wonderful            ADJ\n",
       "12         and          CCONJ\n",
       "13       quick            ADJ\n",
       "14           .          PUNCT\n",
       "15      People           NOUN\n",
       "16        were           VERB\n",
       "17     ranting           VERB\n",
       "18          in            ADP\n",
       "19      raving           VERB\n",
       "20       about            ADP\n",
       "21         pei          PROPN\n",
       "22         wei          PROPN\n",
       "23           ,          PUNCT\n",
       "24           I           PRON\n",
       "25         had           VERB\n",
       "26          to           PART\n",
       "27         try           VERB\n",
       "28          it           PRON\n",
       "29           .          PUNCT\n",
       "..         ...            ...\n",
       "218    refused           VERB\n",
       "219         to           PART\n",
       "220       take           VERB\n",
       "221         it           PRON\n",
       "222       home            ADV\n",
       "223       with            ADP\n",
       "224         us           PRON\n",
       "225          .          PUNCT\n",
       "226                     SPACE\n",
       "227         If            ADP\n",
       "228        you           PRON\n",
       "229       like           VERB\n",
       "230        and          CCONJ\n",
       "231      enjoy           VERB\n",
       "232       what           NOUN\n",
       "233    typical            ADJ\n",
       "234      Asian            ADJ\n",
       "235       food           NOUN\n",
       "236     tastes           NOUN\n",
       "237       like            ADP\n",
       "238          ,          PUNCT\n",
       "239                     SPACE\n",
       "240         do           VERB\n",
       "241        n't            ADV\n",
       "242      waste           VERB\n",
       "243       your            ADJ\n",
       "244       time           NOUN\n",
       "245       here            ADV\n",
       "246          .          PUNCT\n",
       "247         \\n          SPACE\n",
       "\n",
       "[248 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_pos),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text normalization includes converting words to their lemmatized form so that grammatical form is removed in order to simplify and reduce the vocabulary. Hence words are converted to lowercase and words with the same stem but different grammatical forms are converted to a single lemma (e.g. \"be:, \"is\", \"were\", \"am\", \"are\" are all represented as \"be\" in lemmatized form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>the</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td>staff</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great</td>\n",
       "      <td>great</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'re</td>\n",
       "      <td>be</td>\n",
       "      <td>'xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wonderful</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quick</td>\n",
       "      <td>quick</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>People</td>\n",
       "      <td>people</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>were</td>\n",
       "      <td>be</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ranting</td>\n",
       "      <td>rant</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>raving</td>\n",
       "      <td>rave</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pei</td>\n",
       "      <td>pei</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wei</td>\n",
       "      <td>wei</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>had</td>\n",
       "      <td>have</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>try</td>\n",
       "      <td>try</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>refused</td>\n",
       "      <td>refuse</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>take</td>\n",
       "      <td>take</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>it</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>us</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>If</td>\n",
       "      <td>if</td>\n",
       "      <td>Xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>you</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>enjoy</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>what</td>\n",
       "      <td>what</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>typical</td>\n",
       "      <td>typical</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Asian</td>\n",
       "      <td>asian</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>food</td>\n",
       "      <td>food</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>tastes</td>\n",
       "      <td>taste</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>do</td>\n",
       "      <td>do</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>n't</td>\n",
       "      <td>not</td>\n",
       "      <td>x'x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>waste</td>\n",
       "      <td>waste</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>your</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>time</td>\n",
       "      <td>time</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text token_lemma token_shape\n",
       "0          The         the         Xxx\n",
       "1        staff       staff        xxxx\n",
       "2         here        here        xxxx\n",
       "3           is          be          xx\n",
       "4        great       great        xxxx\n",
       "5          and         and         xxx\n",
       "6         they      -PRON-        xxxx\n",
       "7          're          be         'xx\n",
       "8         nice        nice        xxxx\n",
       "9            ,           ,           ,\n",
       "10                                    \n",
       "11   wonderful   wonderful        xxxx\n",
       "12         and         and         xxx\n",
       "13       quick       quick        xxxx\n",
       "14           .           .           .\n",
       "15      People      people       Xxxxx\n",
       "16        were          be        xxxx\n",
       "17     ranting        rant        xxxx\n",
       "18          in          in          xx\n",
       "19      raving        rave        xxxx\n",
       "20       about       about        xxxx\n",
       "21         pei         pei         xxx\n",
       "22         wei         wei         xxx\n",
       "23           ,           ,           ,\n",
       "24           I      -PRON-           X\n",
       "25         had        have         xxx\n",
       "26          to          to          xx\n",
       "27         try         try         xxx\n",
       "28          it      -PRON-          xx\n",
       "29           .           .           .\n",
       "..         ...         ...         ...\n",
       "218    refused      refuse        xxxx\n",
       "219         to          to          xx\n",
       "220       take        take        xxxx\n",
       "221         it      -PRON-          xx\n",
       "222       home        home        xxxx\n",
       "223       with        with        xxxx\n",
       "224         us      -PRON-          xx\n",
       "225          .           .           .\n",
       "226                                   \n",
       "227         If          if          Xx\n",
       "228        you      -PRON-         xxx\n",
       "229       like        like        xxxx\n",
       "230        and         and         xxx\n",
       "231      enjoy       enjoy        xxxx\n",
       "232       what        what        xxxx\n",
       "233    typical     typical        xxxx\n",
       "234      Asian       asian       Xxxxx\n",
       "235       food        food        xxxx\n",
       "236     tastes       taste        xxxx\n",
       "237       like        like        xxxx\n",
       "238          ,           ,           ,\n",
       "239                                   \n",
       "240         do          do          xx\n",
       "241        n't         not         x'x\n",
       "242      waste       waste        xxxx\n",
       "243       your      -PRON-        xxxx\n",
       "244       time        time        xxxx\n",
       "245       here        here        xxxx\n",
       "246          .           .           .\n",
       "247         \\n          \\n          \\n\n",
       "\n",
       "[248 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Phrase modeling_ is an algorithm that passes over the text corpus to learn multi-word concepts. Using the algorithm once will detect phrases such as \"ice cream\" by detecting that \"ice\" and \"cream\" appear together with a frequency that passes a certain threshold. At the next pass, the algorithm will treat the bigram \"ice_cream\" as a single token and may also learn \"vanilla_ice_cream\" as a phrase and treat it as a trigram. \n",
    "\n",
    "This part makes three passes over the text corpus in order to reveal trigrams, which will help provide a robust level of meaning, given that the corpus is fairly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Work\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using CNTK backend\n",
      "C:\\Users\\Work\\Anaconda2\\lib\\site-packages\\keras\\backend\\cntk_backend.py:19: UserWarning: CNTK backend warning: GPU is not detected. CNTK's CPU version is not fully optimized,please run with GPU to get better performance.\n",
      "  'CNTK backend warning: GPU is not detected. '\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n",
    "\n",
    "1. Segment text of complete reviews into sentences & normalize text\n",
    "1. First-order phrase modeling $\\rightarrow$ _apply first-order phrase model to transform sentences_\n",
    "1. Second-order phrase modeling $\\rightarrow$ _apply second-order phrase model to transform sentences_\n",
    "1. Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "We'll use this transformed data as the input for some higher-level modeling approaches in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lemmatized_sentence_corpus` generator loops over the original review text, segmenting the reviews into individual sentences and normalizing the text. The output is written to a new file (`unigram_sentences_all`), with one normalized sentence per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next phase passes over the text to link individual words into two-word phrases e.g. linking \"`ice cream`\" into a new, single token: \"`ice_cream`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "\n",
    "if 0 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences, max_vocab_size=10000000)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "# warning message: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                      'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences, max_vocab_size=10000000)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step comprises the following: \n",
    "- running the complete text of the reviews through a pipeline that applies text normalization and phrase models\n",
    "- removing stopwords\n",
    "- writing the output to a new file with one review per line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.en.language_data.STOP_WORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (_LDA_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import warnings\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory,\n",
    "                                           'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory,\n",
    "                                    'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 971 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "if 0 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_reviews_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 344 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is completed\n",
    "if 0 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print u'{:20} {}'.format(u'term', u'frequency') + u'\\n'\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print u'{:20} {:.3f}'.format(term, round(frequency, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "review               0.062\n",
      "yelp                 0.023\n",
      "know                 0.020\n",
      "read                 0.020\n",
      "write                0.014\n",
      "guy                  0.011\n",
      "people               0.010\n",
      "like                 0.010\n",
      "man                  0.009\n",
      "let                  0.009\n",
      "think                0.009\n",
      "u                    0.008\n",
      "want                 0.008\n",
      "'s                   0.007\n",
      "post                 0.007\n",
      "groupon              0.007\n",
      "star                 0.006\n",
      "use                  0.006\n",
      "mean                 0.005\n",
      "base                 0.005\n",
      "yes                  0.005\n",
      "way                  0.005\n",
      "question             0.005\n",
      "update               0.004\n",
      "girl                 0.004\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the text corpus grouped into clusters, we can explore the words in each cluster to identify a common theme/topic. The list below attempts to identify a common-sense title for each cluster. We can then use this to summarize a review in terms of 3-5 key topics discussed by the reviewer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = {0: u'dessert',\n",
    "               1: u'table, ordering & service',\n",
    "               2: u'airport & flying, allergies',\n",
    "               3: u'buffet & dishes',\n",
    "               4: u'pizza & italian',\n",
    "               5: u'bar & drinking',\n",
    "               6: u'salad & lunch',\n",
    "               7: u'kids & family',\n",
    "               8: u'las vegas',\n",
    "               9: u'fish & asian',\n",
    "               10: u'seating experience',\n",
    "               11: u'waffle / cupcake', # waffle + various unrelated terms\n",
    "               12: u'family members',\n",
    "               13: u'flavor & food experience',\n",
    "               14: u'general ambience',\n",
    "               15: u'night / happy hour / drinks',\n",
    "               16: u'brunch',\n",
    "               17: u'mexican',\n",
    "               18: u'meat',\n",
    "               19: u'price & paying',\n",
    "               20: u'wedding / german', # various unrelated words incl single letters\n",
    "               21: u'various locations', # e.g. north, south, town names, uptown, street etc.\n",
    "               22: u'cleansiness',\n",
    "               23: u'mall / cheesecake_factory', # plus various unrelated words and some experience words\n",
    "               24: u'positive service experience',\n",
    "               25: u'buying / offers',\n",
    "               26: u'positive food experience',\n",
    "               27: u'cheese / wings',\n",
    "               28: u'french language reviews',\n",
    "               29: u'positive experience',\n",
    "               30: u'ordering and service', # geenral and negative service experience\n",
    "               31: u'time & place',\n",
    "               32: u'menu & dishes',\n",
    "               33: u'sushi & fish',\n",
    "               34: u'german language reviews',\n",
    "               35: u'positive food experience',\n",
    "               36: u'timing & waiting',\n",
    "               37: u'seafood',\n",
    "               38: u'tea & drinks',\n",
    "               39: u'thai',\n",
    "               40: u'staff', \n",
    "               41: u'sandwiches',\n",
    "               42: u'yes / know / eat', # unrelated common words\n",
    "               43: u'hot_dog, chilli', #hot is the most prominent word, both in teh sense of spicy and hot_dog\n",
    "               44: u'drinks / coffee / atmosphere',\n",
    "               45: u'burgers',\n",
    "               46: u'breakfast food', \n",
    "               47: u'price / service / portions',\n",
    "               48: u'steaks',\n",
    "               49: u'review / yelp'} # various common urelated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'w') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a variety of topics related to different types of restaurant, such as **pizza&italian**, **mexican**, **thai**, **sushi&fish**, etc., as well as topics related to the restaurant experience such as seating, service and pricing. \n",
    "\n",
    "Beyond these two categories, there are still some topics that are not straightforward to interpret, such as topics 23 and 42.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash; they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Here we will use the _gensim_ [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf) model originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "One complete pass sliding the window across all of the corpus text is known as a training *epoch*. We will train the model over multiple passes/epochs in order to optimize the vector representations that are *close* to each other in vector space.\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors - we will use 100-dimensional vector space to allow sufficient complexity at reasonable computing cost\n",
    "- The width of the sliding window, in tokens - default is used\n",
    "- The number of training epochs - we will use 5 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section trains the word2vec model using the normalized sentences of the tri-gram phrase model in 100-dimensional vector space, using 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 training epochs so far.\n"
     ]
    }
   ],
   "source": [
    "if 0 ==1:\n",
    "    food2vec.train(trigram_sentences, total_examples=food2vec.corpus_count, epochs=food2vec.iter)\n",
    "    print food2vec.train_count\n",
    "    food2vec.save(word2vec_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print u'{} training epochs so far.'.format(food2vec.train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print food2vec.iter\n",
    "print food2vec.train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80,588 terms in the food2vec vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the word vector model, we can create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-PRON-</th>\n",
       "      <td>-0.075121</td>\n",
       "      <td>-0.067430</td>\n",
       "      <td>-0.127061</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>-0.022045</td>\n",
       "      <td>0.170232</td>\n",
       "      <td>0.160691</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.201154</td>\n",
       "      <td>-0.037751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080210</td>\n",
       "      <td>-0.049757</td>\n",
       "      <td>0.025534</td>\n",
       "      <td>0.015274</td>\n",
       "      <td>-0.086829</td>\n",
       "      <td>-0.124214</td>\n",
       "      <td>0.064420</td>\n",
       "      <td>0.040135</td>\n",
       "      <td>-0.064753</td>\n",
       "      <td>0.124752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>0.012676</td>\n",
       "      <td>-0.047068</td>\n",
       "      <td>-0.190982</td>\n",
       "      <td>-0.033070</td>\n",
       "      <td>0.053919</td>\n",
       "      <td>0.250453</td>\n",
       "      <td>0.093323</td>\n",
       "      <td>-0.045521</td>\n",
       "      <td>0.229455</td>\n",
       "      <td>0.075257</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169080</td>\n",
       "      <td>-0.003657</td>\n",
       "      <td>-0.147224</td>\n",
       "      <td>-0.002735</td>\n",
       "      <td>-0.115380</td>\n",
       "      <td>-0.210662</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.041456</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>-0.022719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.114931</td>\n",
       "      <td>0.122418</td>\n",
       "      <td>-0.168722</td>\n",
       "      <td>0.071732</td>\n",
       "      <td>-0.059859</td>\n",
       "      <td>0.315316</td>\n",
       "      <td>0.036166</td>\n",
       "      <td>-0.029906</td>\n",
       "      <td>0.292869</td>\n",
       "      <td>-0.101682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023712</td>\n",
       "      <td>-0.109440</td>\n",
       "      <td>-0.081754</td>\n",
       "      <td>0.105876</td>\n",
       "      <td>-0.073468</td>\n",
       "      <td>-0.165876</td>\n",
       "      <td>0.151434</td>\n",
       "      <td>0.016965</td>\n",
       "      <td>-0.156994</td>\n",
       "      <td>0.009750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.114759</td>\n",
       "      <td>-0.138922</td>\n",
       "      <td>-0.144100</td>\n",
       "      <td>0.006292</td>\n",
       "      <td>0.117541</td>\n",
       "      <td>0.288687</td>\n",
       "      <td>0.099160</td>\n",
       "      <td>0.027657</td>\n",
       "      <td>0.343955</td>\n",
       "      <td>0.049567</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030788</td>\n",
       "      <td>-0.053569</td>\n",
       "      <td>-0.044293</td>\n",
       "      <td>0.042094</td>\n",
       "      <td>-0.086180</td>\n",
       "      <td>-0.017890</td>\n",
       "      <td>0.075092</td>\n",
       "      <td>-0.055175</td>\n",
       "      <td>-0.031652</td>\n",
       "      <td>0.033878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.032536</td>\n",
       "      <td>-0.074984</td>\n",
       "      <td>0.064469</td>\n",
       "      <td>-0.171038</td>\n",
       "      <td>0.020188</td>\n",
       "      <td>0.134144</td>\n",
       "      <td>0.109164</td>\n",
       "      <td>0.008352</td>\n",
       "      <td>0.244023</td>\n",
       "      <td>0.021498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041942</td>\n",
       "      <td>0.023825</td>\n",
       "      <td>-0.163675</td>\n",
       "      <td>0.137406</td>\n",
       "      <td>-0.065831</td>\n",
       "      <td>-0.101458</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>-0.006488</td>\n",
       "      <td>-0.038800</td>\n",
       "      <td>0.170717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.040521</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>-0.043932</td>\n",
       "      <td>0.044770</td>\n",
       "      <td>-0.113628</td>\n",
       "      <td>-0.022404</td>\n",
       "      <td>0.098611</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>0.223237</td>\n",
       "      <td>-0.003608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111642</td>\n",
       "      <td>-0.035838</td>\n",
       "      <td>0.109139</td>\n",
       "      <td>0.119915</td>\n",
       "      <td>-0.073793</td>\n",
       "      <td>-0.023686</td>\n",
       "      <td>-0.009697</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.104554</td>\n",
       "      <td>0.023787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>-0.084267</td>\n",
       "      <td>-0.004397</td>\n",
       "      <td>-0.139282</td>\n",
       "      <td>-0.078336</td>\n",
       "      <td>0.152177</td>\n",
       "      <td>0.199497</td>\n",
       "      <td>-0.005634</td>\n",
       "      <td>0.043972</td>\n",
       "      <td>0.179384</td>\n",
       "      <td>-0.006454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177000</td>\n",
       "      <td>0.035325</td>\n",
       "      <td>-0.133072</td>\n",
       "      <td>0.059747</td>\n",
       "      <td>-0.014941</td>\n",
       "      <td>0.053005</td>\n",
       "      <td>0.155703</td>\n",
       "      <td>-0.023206</td>\n",
       "      <td>-0.083596</td>\n",
       "      <td>-0.067215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.032294</td>\n",
       "      <td>-0.078110</td>\n",
       "      <td>-0.054774</td>\n",
       "      <td>0.063565</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.204627</td>\n",
       "      <td>0.098279</td>\n",
       "      <td>0.024468</td>\n",
       "      <td>0.223266</td>\n",
       "      <td>0.041319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088307</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>0.040729</td>\n",
       "      <td>0.075636</td>\n",
       "      <td>-0.002357</td>\n",
       "      <td>-0.060602</td>\n",
       "      <td>0.052249</td>\n",
       "      <td>-0.035362</td>\n",
       "      <td>-0.188302</td>\n",
       "      <td>0.044007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>-0.052620</td>\n",
       "      <td>-0.045803</td>\n",
       "      <td>-0.088368</td>\n",
       "      <td>0.092387</td>\n",
       "      <td>-0.012385</td>\n",
       "      <td>-0.009546</td>\n",
       "      <td>0.076239</td>\n",
       "      <td>0.026310</td>\n",
       "      <td>0.149709</td>\n",
       "      <td>0.072366</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033276</td>\n",
       "      <td>-0.031341</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>0.066965</td>\n",
       "      <td>-0.087295</td>\n",
       "      <td>-0.164688</td>\n",
       "      <td>0.060534</td>\n",
       "      <td>0.188567</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>0.041429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0.108099</td>\n",
       "      <td>0.026058</td>\n",
       "      <td>0.099891</td>\n",
       "      <td>-0.059244</td>\n",
       "      <td>-0.024895</td>\n",
       "      <td>0.214824</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>-0.035647</td>\n",
       "      <td>0.049735</td>\n",
       "      <td>-0.035197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053567</td>\n",
       "      <td>-0.012096</td>\n",
       "      <td>-0.098778</td>\n",
       "      <td>0.110779</td>\n",
       "      <td>-0.126084</td>\n",
       "      <td>-0.069208</td>\n",
       "      <td>0.050522</td>\n",
       "      <td>0.049058</td>\n",
       "      <td>-0.100268</td>\n",
       "      <td>0.079133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.041681</td>\n",
       "      <td>-0.015251</td>\n",
       "      <td>-0.094702</td>\n",
       "      <td>0.047722</td>\n",
       "      <td>0.012790</td>\n",
       "      <td>0.168519</td>\n",
       "      <td>-0.055776</td>\n",
       "      <td>-0.197996</td>\n",
       "      <td>0.167648</td>\n",
       "      <td>0.079688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016027</td>\n",
       "      <td>0.138046</td>\n",
       "      <td>0.131412</td>\n",
       "      <td>-0.012072</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.074119</td>\n",
       "      <td>0.112797</td>\n",
       "      <td>-0.041809</td>\n",
       "      <td>-0.079513</td>\n",
       "      <td>-0.071991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>0.231507</td>\n",
       "      <td>0.061956</td>\n",
       "      <td>-0.046551</td>\n",
       "      <td>-0.069648</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.118189</td>\n",
       "      <td>-0.006966</td>\n",
       "      <td>-0.099643</td>\n",
       "      <td>0.211284</td>\n",
       "      <td>0.165497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024873</td>\n",
       "      <td>0.028910</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.174381</td>\n",
       "      <td>-0.071311</td>\n",
       "      <td>0.014584</td>\n",
       "      <td>-0.084852</td>\n",
       "      <td>-0.033206</td>\n",
       "      <td>-0.079724</td>\n",
       "      <td>-0.041860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.047970</td>\n",
       "      <td>-0.115554</td>\n",
       "      <td>0.048977</td>\n",
       "      <td>0.062427</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.177053</td>\n",
       "      <td>0.045978</td>\n",
       "      <td>0.253869</td>\n",
       "      <td>-0.135083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047086</td>\n",
       "      <td>-0.030434</td>\n",
       "      <td>0.006797</td>\n",
       "      <td>0.081248</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>-0.167754</td>\n",
       "      <td>0.175307</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>-0.183197</td>\n",
       "      <td>-0.089717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>-0.153117</td>\n",
       "      <td>-0.052337</td>\n",
       "      <td>-0.165207</td>\n",
       "      <td>0.042091</td>\n",
       "      <td>0.054038</td>\n",
       "      <td>0.159040</td>\n",
       "      <td>0.125125</td>\n",
       "      <td>0.064345</td>\n",
       "      <td>0.226260</td>\n",
       "      <td>-0.064774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036399</td>\n",
       "      <td>0.061405</td>\n",
       "      <td>-0.123044</td>\n",
       "      <td>0.048308</td>\n",
       "      <td>-0.147421</td>\n",
       "      <td>-0.051132</td>\n",
       "      <td>0.143027</td>\n",
       "      <td>0.057177</td>\n",
       "      <td>-0.129424</td>\n",
       "      <td>-0.020618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>-0.166842</td>\n",
       "      <td>0.052196</td>\n",
       "      <td>-0.268700</td>\n",
       "      <td>0.081079</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>0.136617</td>\n",
       "      <td>0.152660</td>\n",
       "      <td>-0.120444</td>\n",
       "      <td>0.123370</td>\n",
       "      <td>0.059022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172629</td>\n",
       "      <td>-0.116435</td>\n",
       "      <td>-0.127564</td>\n",
       "      <td>0.069711</td>\n",
       "      <td>-0.103830</td>\n",
       "      <td>-0.079846</td>\n",
       "      <td>0.009578</td>\n",
       "      <td>-0.009213</td>\n",
       "      <td>-0.078922</td>\n",
       "      <td>0.056002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>-0.048841</td>\n",
       "      <td>0.099526</td>\n",
       "      <td>-0.119173</td>\n",
       "      <td>-0.009151</td>\n",
       "      <td>-0.039752</td>\n",
       "      <td>0.070875</td>\n",
       "      <td>0.114179</td>\n",
       "      <td>0.097202</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.020461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109729</td>\n",
       "      <td>-0.058495</td>\n",
       "      <td>-0.119857</td>\n",
       "      <td>0.192714</td>\n",
       "      <td>-0.046124</td>\n",
       "      <td>-0.225118</td>\n",
       "      <td>0.153466</td>\n",
       "      <td>-0.044794</td>\n",
       "      <td>-0.088931</td>\n",
       "      <td>-0.025346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>-0.145139</td>\n",
       "      <td>-0.095728</td>\n",
       "      <td>-0.174306</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.033305</td>\n",
       "      <td>0.178362</td>\n",
       "      <td>0.038774</td>\n",
       "      <td>0.053131</td>\n",
       "      <td>0.016618</td>\n",
       "      <td>0.062293</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115787</td>\n",
       "      <td>-0.063886</td>\n",
       "      <td>-0.235172</td>\n",
       "      <td>0.043143</td>\n",
       "      <td>-0.092227</td>\n",
       "      <td>-0.117109</td>\n",
       "      <td>0.159203</td>\n",
       "      <td>-0.006079</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>-0.005671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>0.044580</td>\n",
       "      <td>-0.108453</td>\n",
       "      <td>-0.144697</td>\n",
       "      <td>-0.066781</td>\n",
       "      <td>0.030958</td>\n",
       "      <td>0.189751</td>\n",
       "      <td>0.037752</td>\n",
       "      <td>-0.125166</td>\n",
       "      <td>0.017389</td>\n",
       "      <td>-0.035712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011821</td>\n",
       "      <td>0.054055</td>\n",
       "      <td>0.091993</td>\n",
       "      <td>0.097342</td>\n",
       "      <td>-0.024247</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>-0.083258</td>\n",
       "      <td>0.177199</td>\n",
       "      <td>-0.079912</td>\n",
       "      <td>0.052545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>-0.182227</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>-0.232003</td>\n",
       "      <td>0.144108</td>\n",
       "      <td>-0.072934</td>\n",
       "      <td>-0.019829</td>\n",
       "      <td>0.027939</td>\n",
       "      <td>-0.022006</td>\n",
       "      <td>0.185074</td>\n",
       "      <td>-0.033581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041785</td>\n",
       "      <td>0.160770</td>\n",
       "      <td>-0.031583</td>\n",
       "      <td>0.035659</td>\n",
       "      <td>0.102493</td>\n",
       "      <td>-0.005057</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>0.096568</td>\n",
       "      <td>-0.125769</td>\n",
       "      <td>0.014746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>-0.045155</td>\n",
       "      <td>-0.169708</td>\n",
       "      <td>-0.132823</td>\n",
       "      <td>-0.109140</td>\n",
       "      <td>-0.095257</td>\n",
       "      <td>0.025914</td>\n",
       "      <td>-0.131884</td>\n",
       "      <td>0.083668</td>\n",
       "      <td>-0.210232</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.201310</td>\n",
       "      <td>0.044618</td>\n",
       "      <td>-0.249970</td>\n",
       "      <td>0.160110</td>\n",
       "      <td>0.035160</td>\n",
       "      <td>-0.078361</td>\n",
       "      <td>0.048521</td>\n",
       "      <td>0.050932</td>\n",
       "      <td>0.008414</td>\n",
       "      <td>0.000480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>-0.162822</td>\n",
       "      <td>-0.139266</td>\n",
       "      <td>0.008402</td>\n",
       "      <td>-0.044224</td>\n",
       "      <td>0.139534</td>\n",
       "      <td>0.087861</td>\n",
       "      <td>-0.034535</td>\n",
       "      <td>0.043186</td>\n",
       "      <td>0.159784</td>\n",
       "      <td>-0.007785</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.030313</td>\n",
       "      <td>-0.031205</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>-0.068078</td>\n",
       "      <td>-0.054393</td>\n",
       "      <td>0.063075</td>\n",
       "      <td>-0.141343</td>\n",
       "      <td>-0.056523</td>\n",
       "      <td>0.014155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>0.050770</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>-0.159441</td>\n",
       "      <td>-0.017592</td>\n",
       "      <td>0.096542</td>\n",
       "      <td>0.174250</td>\n",
       "      <td>0.056703</td>\n",
       "      <td>-0.098840</td>\n",
       "      <td>0.089789</td>\n",
       "      <td>0.112480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190656</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>-0.114823</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>-0.193399</td>\n",
       "      <td>-0.064437</td>\n",
       "      <td>0.047417</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>-0.143821</td>\n",
       "      <td>0.096789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>0.014729</td>\n",
       "      <td>0.050128</td>\n",
       "      <td>-0.226486</td>\n",
       "      <td>-0.143993</td>\n",
       "      <td>-0.051075</td>\n",
       "      <td>0.078544</td>\n",
       "      <td>-0.011193</td>\n",
       "      <td>0.084671</td>\n",
       "      <td>0.172911</td>\n",
       "      <td>0.141926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030755</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>-0.056464</td>\n",
       "      <td>-0.028093</td>\n",
       "      <td>0.015045</td>\n",
       "      <td>-0.109407</td>\n",
       "      <td>0.073646</td>\n",
       "      <td>-0.148390</td>\n",
       "      <td>-0.078517</td>\n",
       "      <td>0.007968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>0.049503</td>\n",
       "      <td>-0.119424</td>\n",
       "      <td>-0.033875</td>\n",
       "      <td>0.092584</td>\n",
       "      <td>-0.160535</td>\n",
       "      <td>0.230116</td>\n",
       "      <td>-0.094146</td>\n",
       "      <td>-0.155029</td>\n",
       "      <td>0.129601</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>-0.006592</td>\n",
       "      <td>-0.071516</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>-0.062966</td>\n",
       "      <td>-0.055145</td>\n",
       "      <td>0.084811</td>\n",
       "      <td>0.010498</td>\n",
       "      <td>-0.177061</td>\n",
       "      <td>-0.072776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order</th>\n",
       "      <td>0.091938</td>\n",
       "      <td>0.075876</td>\n",
       "      <td>-0.216210</td>\n",
       "      <td>0.031112</td>\n",
       "      <td>-0.025435</td>\n",
       "      <td>0.115223</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.268448</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144986</td>\n",
       "      <td>0.040197</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>-0.073201</td>\n",
       "      <td>-0.161401</td>\n",
       "      <td>-0.006538</td>\n",
       "      <td>0.008493</td>\n",
       "      <td>0.062066</td>\n",
       "      <td>-0.115829</td>\n",
       "      <td>0.089099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>-0.020339</td>\n",
       "      <td>-0.039427</td>\n",
       "      <td>-0.131847</td>\n",
       "      <td>0.084931</td>\n",
       "      <td>0.108922</td>\n",
       "      <td>0.078356</td>\n",
       "      <td>0.044275</td>\n",
       "      <td>0.109155</td>\n",
       "      <td>0.293720</td>\n",
       "      <td>-0.093648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107823</td>\n",
       "      <td>0.066603</td>\n",
       "      <td>-0.099904</td>\n",
       "      <td>0.107366</td>\n",
       "      <td>-0.028048</td>\n",
       "      <td>-0.078120</td>\n",
       "      <td>0.069437</td>\n",
       "      <td>-0.169679</td>\n",
       "      <td>-0.031027</td>\n",
       "      <td>0.039416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.122305</td>\n",
       "      <td>0.036651</td>\n",
       "      <td>-0.199770</td>\n",
       "      <td>0.035668</td>\n",
       "      <td>0.051312</td>\n",
       "      <td>0.112124</td>\n",
       "      <td>-0.010971</td>\n",
       "      <td>-0.123858</td>\n",
       "      <td>0.164580</td>\n",
       "      <td>0.100187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158378</td>\n",
       "      <td>-0.088051</td>\n",
       "      <td>-0.200404</td>\n",
       "      <td>0.098581</td>\n",
       "      <td>-0.097083</td>\n",
       "      <td>0.058702</td>\n",
       "      <td>0.023418</td>\n",
       "      <td>-0.016989</td>\n",
       "      <td>0.032076</td>\n",
       "      <td>0.171971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>0.029132</td>\n",
       "      <td>-0.075060</td>\n",
       "      <td>-0.018703</td>\n",
       "      <td>0.120321</td>\n",
       "      <td>0.095200</td>\n",
       "      <td>0.195629</td>\n",
       "      <td>-0.114325</td>\n",
       "      <td>0.015402</td>\n",
       "      <td>0.263375</td>\n",
       "      <td>0.020072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139809</td>\n",
       "      <td>-0.074991</td>\n",
       "      <td>-0.125387</td>\n",
       "      <td>0.037095</td>\n",
       "      <td>-0.118574</td>\n",
       "      <td>-0.091542</td>\n",
       "      <td>0.025768</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>-0.054532</td>\n",
       "      <td>0.126413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there</th>\n",
       "      <td>-0.058092</td>\n",
       "      <td>-0.046277</td>\n",
       "      <td>-0.001933</td>\n",
       "      <td>0.072566</td>\n",
       "      <td>-0.031244</td>\n",
       "      <td>0.133463</td>\n",
       "      <td>-0.138294</td>\n",
       "      <td>0.056258</td>\n",
       "      <td>0.135409</td>\n",
       "      <td>0.006977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.094851</td>\n",
       "      <td>-0.153519</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>-0.109259</td>\n",
       "      <td>0.065827</td>\n",
       "      <td>0.056533</td>\n",
       "      <td>-0.031071</td>\n",
       "      <td>-0.125726</td>\n",
       "      <td>-0.216364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.051280</td>\n",
       "      <td>-0.042952</td>\n",
       "      <td>-0.046526</td>\n",
       "      <td>0.063202</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0.068441</td>\n",
       "      <td>0.234513</td>\n",
       "      <td>0.043554</td>\n",
       "      <td>0.170928</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080010</td>\n",
       "      <td>0.170374</td>\n",
       "      <td>-0.128055</td>\n",
       "      <td>0.247395</td>\n",
       "      <td>-0.112259</td>\n",
       "      <td>-0.018157</td>\n",
       "      <td>-0.013934</td>\n",
       "      <td>0.059989</td>\n",
       "      <td>-0.122854</td>\n",
       "      <td>0.064167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>active_duty_military</th>\n",
       "      <td>0.075698</td>\n",
       "      <td>0.135177</td>\n",
       "      <td>-0.154208</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>-0.149845</td>\n",
       "      <td>-0.061751</td>\n",
       "      <td>0.035960</td>\n",
       "      <td>0.139171</td>\n",
       "      <td>0.075327</td>\n",
       "      <td>0.130673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083939</td>\n",
       "      <td>-0.010948</td>\n",
       "      <td>-0.158681</td>\n",
       "      <td>0.009105</td>\n",
       "      <td>0.071534</td>\n",
       "      <td>0.081744</td>\n",
       "      <td>0.050954</td>\n",
       "      <td>0.092710</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>0.044877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>santana_row</th>\n",
       "      <td>0.071118</td>\n",
       "      <td>-0.063849</td>\n",
       "      <td>-0.171206</td>\n",
       "      <td>0.093245</td>\n",
       "      <td>-0.114105</td>\n",
       "      <td>0.015191</td>\n",
       "      <td>-0.131874</td>\n",
       "      <td>0.042730</td>\n",
       "      <td>-0.196989</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088767</td>\n",
       "      <td>-0.028912</td>\n",
       "      <td>-0.079247</td>\n",
       "      <td>0.021594</td>\n",
       "      <td>-0.027070</td>\n",
       "      <td>-0.080100</td>\n",
       "      <td>0.132137</td>\n",
       "      <td>-0.079408</td>\n",
       "      <td>-0.118926</td>\n",
       "      <td>0.063933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seltsame</th>\n",
       "      <td>-0.091998</td>\n",
       "      <td>0.020160</td>\n",
       "      <td>-0.204461</td>\n",
       "      <td>-0.107606</td>\n",
       "      <td>-0.211536</td>\n",
       "      <td>-0.019701</td>\n",
       "      <td>-0.031341</td>\n",
       "      <td>0.027758</td>\n",
       "      <td>-0.116761</td>\n",
       "      <td>0.052492</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036584</td>\n",
       "      <td>-0.085108</td>\n",
       "      <td>0.080140</td>\n",
       "      <td>-0.126538</td>\n",
       "      <td>-0.095934</td>\n",
       "      <td>0.040602</td>\n",
       "      <td>-0.004856</td>\n",
       "      <td>0.034584</td>\n",
       "      <td>-0.093786</td>\n",
       "      <td>0.145502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gastronomisch</th>\n",
       "      <td>-0.044063</td>\n",
       "      <td>0.010626</td>\n",
       "      <td>-0.202993</td>\n",
       "      <td>-0.083327</td>\n",
       "      <td>-0.187213</td>\n",
       "      <td>-0.027528</td>\n",
       "      <td>-0.063387</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>-0.078177</td>\n",
       "      <td>0.059397</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044552</td>\n",
       "      <td>-0.057372</td>\n",
       "      <td>0.051270</td>\n",
       "      <td>-0.161742</td>\n",
       "      <td>-0.096971</td>\n",
       "      <td>-0.044673</td>\n",
       "      <td>-0.003418</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>-0.108050</td>\n",
       "      <td>0.188143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mentionner</th>\n",
       "      <td>-0.055281</td>\n",
       "      <td>0.010328</td>\n",
       "      <td>-0.066787</td>\n",
       "      <td>-0.088453</td>\n",
       "      <td>-0.121454</td>\n",
       "      <td>-0.096231</td>\n",
       "      <td>-0.035907</td>\n",
       "      <td>-0.007050</td>\n",
       "      <td>-0.072055</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070042</td>\n",
       "      <td>-0.121707</td>\n",
       "      <td>-0.025584</td>\n",
       "      <td>-0.093653</td>\n",
       "      <td>-0.026203</td>\n",
       "      <td>0.051164</td>\n",
       "      <td>-0.024464</td>\n",
       "      <td>0.083263</td>\n",
       "      <td>-0.020001</td>\n",
       "      <td>0.170186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tlayuda</th>\n",
       "      <td>-0.111358</td>\n",
       "      <td>-0.033820</td>\n",
       "      <td>-0.161888</td>\n",
       "      <td>-0.047562</td>\n",
       "      <td>0.060807</td>\n",
       "      <td>0.147453</td>\n",
       "      <td>-0.043849</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>-0.135442</td>\n",
       "      <td>0.066431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112881</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.038144</td>\n",
       "      <td>0.109481</td>\n",
       "      <td>-0.052076</td>\n",
       "      <td>-0.028290</td>\n",
       "      <td>-0.114759</td>\n",
       "      <td>-0.049156</td>\n",
       "      <td>-0.055461</td>\n",
       "      <td>0.068606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.98</th>\n",
       "      <td>-0.060488</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.116836</td>\n",
       "      <td>-0.000846</td>\n",
       "      <td>-0.125824</td>\n",
       "      <td>0.126674</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>0.027590</td>\n",
       "      <td>0.171539</td>\n",
       "      <td>0.178407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018818</td>\n",
       "      <td>0.018999</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>0.079557</td>\n",
       "      <td>0.075803</td>\n",
       "      <td>0.028352</td>\n",
       "      <td>0.014360</td>\n",
       "      <td>0.157180</td>\n",
       "      <td>0.025728</td>\n",
       "      <td>0.251373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.8</th>\n",
       "      <td>0.020504</td>\n",
       "      <td>-0.032105</td>\n",
       "      <td>0.030257</td>\n",
       "      <td>0.137854</td>\n",
       "      <td>-0.183497</td>\n",
       "      <td>0.015904</td>\n",
       "      <td>-0.099715</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>0.023165</td>\n",
       "      <td>0.065181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066356</td>\n",
       "      <td>-0.022689</td>\n",
       "      <td>-0.046109</td>\n",
       "      <td>-0.042808</td>\n",
       "      <td>0.064343</td>\n",
       "      <td>-0.037488</td>\n",
       "      <td>-0.100119</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>-0.029452</td>\n",
       "      <td>0.256021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la'wan</th>\n",
       "      <td>0.023135</td>\n",
       "      <td>-0.003220</td>\n",
       "      <td>-0.331206</td>\n",
       "      <td>-0.051811</td>\n",
       "      <td>-0.100299</td>\n",
       "      <td>0.054991</td>\n",
       "      <td>-0.171233</td>\n",
       "      <td>-0.030778</td>\n",
       "      <td>-0.079318</td>\n",
       "      <td>0.098345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151256</td>\n",
       "      <td>-0.057754</td>\n",
       "      <td>-0.123623</td>\n",
       "      <td>0.113851</td>\n",
       "      <td>0.071573</td>\n",
       "      <td>-0.139447</td>\n",
       "      <td>-0.022044</td>\n",
       "      <td>-0.161350</td>\n",
       "      <td>-0.062607</td>\n",
       "      <td>-0.052625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>copoli</th>\n",
       "      <td>-0.087112</td>\n",
       "      <td>0.207826</td>\n",
       "      <td>-0.171917</td>\n",
       "      <td>-0.090752</td>\n",
       "      <td>-0.225479</td>\n",
       "      <td>-0.013626</td>\n",
       "      <td>-0.079290</td>\n",
       "      <td>0.127256</td>\n",
       "      <td>-0.026271</td>\n",
       "      <td>-0.009722</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161245</td>\n",
       "      <td>-0.139925</td>\n",
       "      <td>-0.055061</td>\n",
       "      <td>0.081608</td>\n",
       "      <td>-0.024436</td>\n",
       "      <td>-0.014512</td>\n",
       "      <td>-0.005673</td>\n",
       "      <td>-0.052711</td>\n",
       "      <td>-0.073393</td>\n",
       "      <td>0.130494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfiltered_tap</th>\n",
       "      <td>-0.060695</td>\n",
       "      <td>-0.085672</td>\n",
       "      <td>-0.086453</td>\n",
       "      <td>0.040853</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.028054</td>\n",
       "      <td>0.158814</td>\n",
       "      <td>-0.057763</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>-0.047897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153777</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>-0.059231</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.096502</td>\n",
       "      <td>-0.118470</td>\n",
       "      <td>0.078701</td>\n",
       "      <td>0.165676</td>\n",
       "      <td>0.113941</td>\n",
       "      <td>0.112050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>das_lokal_liegt</th>\n",
       "      <td>-0.032015</td>\n",
       "      <td>-0.017825</td>\n",
       "      <td>-0.165760</td>\n",
       "      <td>-0.092851</td>\n",
       "      <td>-0.189455</td>\n",
       "      <td>-0.067263</td>\n",
       "      <td>-0.118159</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>-0.106028</td>\n",
       "      <td>0.102627</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005626</td>\n",
       "      <td>-0.007377</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.107501</td>\n",
       "      <td>-0.077264</td>\n",
       "      <td>0.015495</td>\n",
       "      <td>0.015058</td>\n",
       "      <td>0.035925</td>\n",
       "      <td>-0.111112</td>\n",
       "      <td>0.197808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shredded_iceberg</th>\n",
       "      <td>-0.028121</td>\n",
       "      <td>-0.112043</td>\n",
       "      <td>-0.024992</td>\n",
       "      <td>-0.097683</td>\n",
       "      <td>-0.094441</td>\n",
       "      <td>0.044758</td>\n",
       "      <td>0.108907</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.114377</td>\n",
       "      <td>0.171440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086990</td>\n",
       "      <td>-0.114200</td>\n",
       "      <td>0.109720</td>\n",
       "      <td>0.071394</td>\n",
       "      <td>-0.088573</td>\n",
       "      <td>-0.082660</td>\n",
       "      <td>-0.112285</td>\n",
       "      <td>0.028193</td>\n",
       "      <td>-0.213291</td>\n",
       "      <td>-0.086563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>panty</th>\n",
       "      <td>-0.059152</td>\n",
       "      <td>0.101745</td>\n",
       "      <td>-0.085401</td>\n",
       "      <td>-0.043494</td>\n",
       "      <td>-0.187712</td>\n",
       "      <td>0.058988</td>\n",
       "      <td>0.066161</td>\n",
       "      <td>-0.070886</td>\n",
       "      <td>-0.117642</td>\n",
       "      <td>0.107217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132313</td>\n",
       "      <td>-0.092307</td>\n",
       "      <td>-0.247743</td>\n",
       "      <td>-0.057379</td>\n",
       "      <td>-0.018347</td>\n",
       "      <td>-0.111343</td>\n",
       "      <td>0.067737</td>\n",
       "      <td>0.167651</td>\n",
       "      <td>0.128654</td>\n",
       "      <td>0.056562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racial_profiling</th>\n",
       "      <td>0.064522</td>\n",
       "      <td>0.093228</td>\n",
       "      <td>-0.106148</td>\n",
       "      <td>0.089736</td>\n",
       "      <td>-0.168122</td>\n",
       "      <td>-0.047535</td>\n",
       "      <td>-0.021964</td>\n",
       "      <td>-0.057480</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>0.068189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>-0.160886</td>\n",
       "      <td>0.065731</td>\n",
       "      <td>0.045111</td>\n",
       "      <td>-0.050579</td>\n",
       "      <td>0.132683</td>\n",
       "      <td>0.034007</td>\n",
       "      <td>-0.045011</td>\n",
       "      <td>-0.190862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>softie</th>\n",
       "      <td>0.077026</td>\n",
       "      <td>0.040195</td>\n",
       "      <td>-0.176717</td>\n",
       "      <td>-0.040961</td>\n",
       "      <td>-0.122685</td>\n",
       "      <td>0.094245</td>\n",
       "      <td>0.105707</td>\n",
       "      <td>0.022846</td>\n",
       "      <td>-0.071944</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094743</td>\n",
       "      <td>-0.072430</td>\n",
       "      <td>-0.021318</td>\n",
       "      <td>0.132094</td>\n",
       "      <td>-0.027889</td>\n",
       "      <td>0.121042</td>\n",
       "      <td>-0.040436</td>\n",
       "      <td>0.132192</td>\n",
       "      <td>0.026054</td>\n",
       "      <td>0.061769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-bite</th>\n",
       "      <td>-0.063560</td>\n",
       "      <td>-0.019889</td>\n",
       "      <td>0.039932</td>\n",
       "      <td>-0.045909</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.140784</td>\n",
       "      <td>0.257919</td>\n",
       "      <td>0.013713</td>\n",
       "      <td>0.111766</td>\n",
       "      <td>-0.091504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117606</td>\n",
       "      <td>0.017328</td>\n",
       "      <td>-0.014158</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>-0.104912</td>\n",
       "      <td>0.035710</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>0.111243</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>0.105992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stancatos</th>\n",
       "      <td>0.098584</td>\n",
       "      <td>0.131954</td>\n",
       "      <td>-0.179151</td>\n",
       "      <td>-0.121466</td>\n",
       "      <td>-0.180906</td>\n",
       "      <td>-0.057148</td>\n",
       "      <td>-0.006942</td>\n",
       "      <td>-0.092197</td>\n",
       "      <td>-0.121852</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122582</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>-0.185685</td>\n",
       "      <td>0.073217</td>\n",
       "      <td>0.038770</td>\n",
       "      <td>-0.002868</td>\n",
       "      <td>0.116158</td>\n",
       "      <td>-0.005037</td>\n",
       "      <td>0.102094</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>si_c'était</th>\n",
       "      <td>-0.039697</td>\n",
       "      <td>-0.047400</td>\n",
       "      <td>-0.052664</td>\n",
       "      <td>-0.063865</td>\n",
       "      <td>-0.143274</td>\n",
       "      <td>-0.107894</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.029121</td>\n",
       "      <td>-0.129268</td>\n",
       "      <td>-0.050664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051624</td>\n",
       "      <td>-0.094695</td>\n",
       "      <td>-0.050911</td>\n",
       "      <td>-0.107928</td>\n",
       "      <td>-0.019283</td>\n",
       "      <td>0.007699</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>-0.008725</td>\n",
       "      <td>-0.049221</td>\n",
       "      <td>0.166984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rustikale_einrichtung</th>\n",
       "      <td>-0.015548</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>-0.195021</td>\n",
       "      <td>-0.099418</td>\n",
       "      <td>-0.219798</td>\n",
       "      <td>-0.031700</td>\n",
       "      <td>-0.056725</td>\n",
       "      <td>-0.016485</td>\n",
       "      <td>-0.142037</td>\n",
       "      <td>0.031182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009180</td>\n",
       "      <td>-0.062048</td>\n",
       "      <td>0.063408</td>\n",
       "      <td>-0.118905</td>\n",
       "      <td>-0.092025</td>\n",
       "      <td>0.004460</td>\n",
       "      <td>-0.051941</td>\n",
       "      <td>0.035309</td>\n",
       "      <td>-0.106882</td>\n",
       "      <td>0.173018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roger_clyne</th>\n",
       "      <td>-0.007507</td>\n",
       "      <td>0.017789</td>\n",
       "      <td>-0.126199</td>\n",
       "      <td>-0.037009</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.136911</td>\n",
       "      <td>-0.143118</td>\n",
       "      <td>-0.028211</td>\n",
       "      <td>0.038061</td>\n",
       "      <td>0.142315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208557</td>\n",
       "      <td>-0.220399</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>-0.083523</td>\n",
       "      <td>-0.018991</td>\n",
       "      <td>-0.051335</td>\n",
       "      <td>0.049166</td>\n",
       "      <td>0.052005</td>\n",
       "      <td>0.080303</td>\n",
       "      <td>-0.032558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buerre</th>\n",
       "      <td>0.037948</td>\n",
       "      <td>0.091147</td>\n",
       "      <td>-0.188829</td>\n",
       "      <td>-0.175427</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.087922</td>\n",
       "      <td>0.030625</td>\n",
       "      <td>0.058320</td>\n",
       "      <td>0.140025</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080017</td>\n",
       "      <td>-0.042219</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>-0.062098</td>\n",
       "      <td>-0.006033</td>\n",
       "      <td>-0.100040</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>-0.035691</td>\n",
       "      <td>0.004768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16_oz_t_bone</th>\n",
       "      <td>-0.089905</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>-0.027825</td>\n",
       "      <td>-0.104140</td>\n",
       "      <td>-0.094412</td>\n",
       "      <td>0.150370</td>\n",
       "      <td>-0.059488</td>\n",
       "      <td>0.087915</td>\n",
       "      <td>0.211372</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077491</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>-0.203505</td>\n",
       "      <td>-0.091067</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>-0.065058</td>\n",
       "      <td>-0.096854</td>\n",
       "      <td>0.013286</td>\n",
       "      <td>-0.101238</td>\n",
       "      <td>0.305150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il_faudra</th>\n",
       "      <td>-0.081382</td>\n",
       "      <td>0.008569</td>\n",
       "      <td>-0.102238</td>\n",
       "      <td>-0.051026</td>\n",
       "      <td>-0.131413</td>\n",
       "      <td>-0.022638</td>\n",
       "      <td>-0.016636</td>\n",
       "      <td>-0.025979</td>\n",
       "      <td>-0.101953</td>\n",
       "      <td>-0.005451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010669</td>\n",
       "      <td>-0.086036</td>\n",
       "      <td>-0.023875</td>\n",
       "      <td>-0.119804</td>\n",
       "      <td>-0.053642</td>\n",
       "      <td>0.053871</td>\n",
       "      <td>-0.045014</td>\n",
       "      <td>0.077664</td>\n",
       "      <td>-0.003368</td>\n",
       "      <td>0.183753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homefry</th>\n",
       "      <td>0.092889</td>\n",
       "      <td>0.011960</td>\n",
       "      <td>-0.100922</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>-0.010852</td>\n",
       "      <td>0.116570</td>\n",
       "      <td>0.282813</td>\n",
       "      <td>0.126190</td>\n",
       "      <td>0.150699</td>\n",
       "      <td>0.032790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027425</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>-0.011670</td>\n",
       "      <td>0.128980</td>\n",
       "      <td>-0.020525</td>\n",
       "      <td>-0.117377</td>\n",
       "      <td>0.136066</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>-0.114365</td>\n",
       "      <td>0.105661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rum_runners</th>\n",
       "      <td>0.115184</td>\n",
       "      <td>0.088051</td>\n",
       "      <td>-0.230919</td>\n",
       "      <td>-0.018992</td>\n",
       "      <td>-0.089586</td>\n",
       "      <td>-0.094877</td>\n",
       "      <td>-0.134352</td>\n",
       "      <td>-0.016583</td>\n",
       "      <td>-0.050102</td>\n",
       "      <td>0.043453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262772</td>\n",
       "      <td>-0.029052</td>\n",
       "      <td>-0.054634</td>\n",
       "      <td>0.101642</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>-0.134599</td>\n",
       "      <td>0.142007</td>\n",
       "      <td>0.020505</td>\n",
       "      <td>0.034816</td>\n",
       "      <td>-0.129387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brust</th>\n",
       "      <td>0.008545</td>\n",
       "      <td>0.031476</td>\n",
       "      <td>-0.154556</td>\n",
       "      <td>-0.091132</td>\n",
       "      <td>-0.211312</td>\n",
       "      <td>0.092016</td>\n",
       "      <td>-0.072978</td>\n",
       "      <td>0.014328</td>\n",
       "      <td>-0.076084</td>\n",
       "      <td>0.027863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016706</td>\n",
       "      <td>-0.043750</td>\n",
       "      <td>0.083264</td>\n",
       "      <td>-0.001440</td>\n",
       "      <td>-0.034947</td>\n",
       "      <td>0.045480</td>\n",
       "      <td>0.031839</td>\n",
       "      <td>-0.042802</td>\n",
       "      <td>-0.066962</td>\n",
       "      <td>0.128032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>würzen</th>\n",
       "      <td>-0.074187</td>\n",
       "      <td>-0.015204</td>\n",
       "      <td>-0.186387</td>\n",
       "      <td>-0.093205</td>\n",
       "      <td>-0.240767</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.071786</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.116675</td>\n",
       "      <td>0.053546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028043</td>\n",
       "      <td>-0.046346</td>\n",
       "      <td>0.081733</td>\n",
       "      <td>-0.150837</td>\n",
       "      <td>-0.100034</td>\n",
       "      <td>0.015725</td>\n",
       "      <td>-0.059749</td>\n",
       "      <td>0.053818</td>\n",
       "      <td>-0.087188</td>\n",
       "      <td>0.158437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>st_martin</th>\n",
       "      <td>0.069025</td>\n",
       "      <td>0.027704</td>\n",
       "      <td>-0.116372</td>\n",
       "      <td>-0.037505</td>\n",
       "      <td>-0.063841</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>-0.105143</td>\n",
       "      <td>0.053471</td>\n",
       "      <td>-0.125264</td>\n",
       "      <td>0.013906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120109</td>\n",
       "      <td>-0.048794</td>\n",
       "      <td>-0.115922</td>\n",
       "      <td>0.065895</td>\n",
       "      <td>0.045484</td>\n",
       "      <td>-0.049816</td>\n",
       "      <td>-0.017363</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>-0.092770</td>\n",
       "      <td>0.100528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>verweilen_ein</th>\n",
       "      <td>-0.038613</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>-0.201094</td>\n",
       "      <td>-0.102596</td>\n",
       "      <td>-0.207095</td>\n",
       "      <td>-0.053924</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>-0.125484</td>\n",
       "      <td>0.077436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029124</td>\n",
       "      <td>-0.046472</td>\n",
       "      <td>0.044894</td>\n",
       "      <td>-0.127964</td>\n",
       "      <td>-0.088891</td>\n",
       "      <td>-0.000579</td>\n",
       "      <td>0.011269</td>\n",
       "      <td>0.065944</td>\n",
       "      <td>-0.083186</td>\n",
       "      <td>0.149842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80588 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4   \\\n",
       "-PRON-                -0.075121 -0.067430 -0.127061 -0.009218 -0.022045   \n",
       "be                     0.012676 -0.047068 -0.190982 -0.033070  0.053919   \n",
       "the                    0.114931  0.122418 -0.168722  0.071732 -0.059859   \n",
       "and                    0.114759 -0.138922 -0.144100  0.006292  0.117541   \n",
       "a                     -0.032536 -0.074984  0.064469 -0.171038  0.020188   \n",
       "to                     0.040521  0.001736 -0.043932  0.044770 -0.113628   \n",
       "have                  -0.084267 -0.004397 -0.139282 -0.078336  0.152177   \n",
       "of                     0.032294 -0.078110 -0.054774  0.063565  0.002737   \n",
       "not                   -0.052620 -0.045803 -0.088368  0.092387 -0.012385   \n",
       "for                    0.108099  0.026058  0.099891 -0.059244 -0.024895   \n",
       "in                     0.041681 -0.015251 -0.094702  0.047722  0.012790   \n",
       "with                   0.231507  0.061956 -0.046551 -0.069648  0.125986   \n",
       "that                   0.008432  0.047970 -0.115554  0.048977  0.062427   \n",
       "but                   -0.153117 -0.052337 -0.165207  0.042091  0.054038   \n",
       "good                  -0.166842  0.052196 -0.268700  0.081079 -0.000356   \n",
       "this                  -0.048841  0.099526 -0.119173 -0.009151 -0.039752   \n",
       "food                  -0.145139 -0.095728 -0.174306  0.003888  0.033305   \n",
       "on                     0.044580 -0.108453 -0.144697 -0.066781  0.030958   \n",
       "do                    -0.182227  0.005094 -0.232003  0.144108 -0.072934   \n",
       "place                 -0.045155 -0.169708 -0.132823 -0.109140 -0.095257   \n",
       "so                    -0.162822 -0.139266  0.008402 -0.044224  0.139534   \n",
       "get                    0.050770  0.043213 -0.159441 -0.017592  0.096542   \n",
       "go                     0.014729  0.050128 -0.226486 -0.143993 -0.051075   \n",
       "at                     0.049503 -0.119424 -0.033875  0.092584 -0.160535   \n",
       "order                  0.091938  0.075876 -0.216210  0.031112 -0.025435   \n",
       "as                    -0.020339 -0.039427 -0.131847  0.084931  0.108922   \n",
       "great                  0.122305  0.036651 -0.199770  0.035668  0.051312   \n",
       "very                   0.029132 -0.075060 -0.018703  0.120321  0.095200   \n",
       "there                 -0.058092 -0.046277 -0.001933  0.072566 -0.031244   \n",
       "like                   0.051280 -0.042952 -0.046526  0.063202  0.006105   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "active_duty_military   0.075698  0.135177 -0.154208  0.000452 -0.149845   \n",
       "santana_row            0.071118 -0.063849 -0.171206  0.093245 -0.114105   \n",
       "seltsame              -0.091998  0.020160 -0.204461 -0.107606 -0.211536   \n",
       "gastronomisch         -0.044063  0.010626 -0.202993 -0.083327 -0.187213   \n",
       "mentionner            -0.055281  0.010328 -0.066787 -0.088453 -0.121454   \n",
       "tlayuda               -0.111358 -0.033820 -0.161888 -0.047562  0.060807   \n",
       "12.98                 -0.060488  0.009333  0.116836 -0.000846 -0.125824   \n",
       "1.8                    0.020504 -0.032105  0.030257  0.137854 -0.183497   \n",
       "la'wan                 0.023135 -0.003220 -0.331206 -0.051811 -0.100299   \n",
       "copoli                -0.087112  0.207826 -0.171917 -0.090752 -0.225479   \n",
       "unfiltered_tap        -0.060695 -0.085672 -0.086453  0.040853  0.001239   \n",
       "das_lokal_liegt       -0.032015 -0.017825 -0.165760 -0.092851 -0.189455   \n",
       "shredded_iceberg      -0.028121 -0.112043 -0.024992 -0.097683 -0.094441   \n",
       "panty                 -0.059152  0.101745 -0.085401 -0.043494 -0.187712   \n",
       "racial_profiling       0.064522  0.093228 -0.106148  0.089736 -0.168122   \n",
       "softie                 0.077026  0.040195 -0.176717 -0.040961 -0.122685   \n",
       "2-bite                -0.063560 -0.019889  0.039932 -0.045909  0.009449   \n",
       "stancatos              0.098584  0.131954 -0.179151 -0.121466 -0.180906   \n",
       "si_c'était            -0.039697 -0.047400 -0.052664 -0.063865 -0.143274   \n",
       "rustikale_einrichtung -0.015548  0.002233 -0.195021 -0.099418 -0.219798   \n",
       "roger_clyne           -0.007507  0.017789 -0.126199 -0.037009 -0.001208   \n",
       "buerre                 0.037948  0.091147 -0.188829 -0.175427  0.001156   \n",
       "16_oz_t_bone          -0.089905  0.001664 -0.027825 -0.104140 -0.094412   \n",
       "il_faudra             -0.081382  0.008569 -0.102238 -0.051026 -0.131413   \n",
       "homefry                0.092889  0.011960 -0.100922  0.019998 -0.010852   \n",
       "rum_runners            0.115184  0.088051 -0.230919 -0.018992 -0.089586   \n",
       "brust                  0.008545  0.031476 -0.154556 -0.091132 -0.211312   \n",
       "würzen                -0.074187 -0.015204 -0.186387 -0.093205 -0.240767   \n",
       "st_martin              0.069025  0.027704 -0.116372 -0.037505 -0.063841   \n",
       "verweilen_ein         -0.038613 -0.023389 -0.201094 -0.102596 -0.207095   \n",
       "\n",
       "                             5         6         7         8         9   \\\n",
       "-PRON-                 0.170232  0.160691  0.043578  0.201154 -0.037751   \n",
       "be                     0.250453  0.093323 -0.045521  0.229455  0.075257   \n",
       "the                    0.315316  0.036166 -0.029906  0.292869 -0.101682   \n",
       "and                    0.288687  0.099160  0.027657  0.343955  0.049567   \n",
       "a                      0.134144  0.109164  0.008352  0.244023  0.021498   \n",
       "to                    -0.022404  0.098611 -0.007043  0.223237 -0.003608   \n",
       "have                   0.199497 -0.005634  0.043972  0.179384 -0.006454   \n",
       "of                     0.204627  0.098279  0.024468  0.223266  0.041319   \n",
       "not                   -0.009546  0.076239  0.026310  0.149709  0.072366   \n",
       "for                    0.214824  0.016041 -0.035647  0.049735 -0.035197   \n",
       "in                     0.168519 -0.055776 -0.197996  0.167648  0.079688   \n",
       "with                   0.118189 -0.006966 -0.099643  0.211284  0.165497   \n",
       "that                   0.053571  0.177053  0.045978  0.253869 -0.135083   \n",
       "but                    0.159040  0.125125  0.064345  0.226260 -0.064774   \n",
       "good                   0.136617  0.152660 -0.120444  0.123370  0.059022   \n",
       "this                   0.070875  0.114179  0.097202  0.035828  0.020461   \n",
       "food                   0.178362  0.038774  0.053131  0.016618  0.062293   \n",
       "on                     0.189751  0.037752 -0.125166  0.017389 -0.035712   \n",
       "do                    -0.019829  0.027939 -0.022006  0.185074 -0.033581   \n",
       "place                  0.025914 -0.131884  0.083668 -0.210232  0.005903   \n",
       "so                     0.087861 -0.034535  0.043186  0.159784 -0.007785   \n",
       "get                    0.174250  0.056703 -0.098840  0.089789  0.112480   \n",
       "go                     0.078544 -0.011193  0.084671  0.172911  0.141926   \n",
       "at                     0.230116 -0.094146 -0.155029  0.129601  0.034301   \n",
       "order                  0.115223  0.130943  0.003790  0.268448  0.003431   \n",
       "as                     0.078356  0.044275  0.109155  0.293720 -0.093648   \n",
       "great                  0.112124 -0.010971 -0.123858  0.164580  0.100187   \n",
       "very                   0.195629 -0.114325  0.015402  0.263375  0.020072   \n",
       "there                  0.133463 -0.138294  0.056258  0.135409  0.006977   \n",
       "like                   0.068441  0.234513  0.043554  0.170928  0.003284   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "active_duty_military  -0.061751  0.035960  0.139171  0.075327  0.130673   \n",
       "santana_row            0.015191 -0.131874  0.042730 -0.196989  0.012964   \n",
       "seltsame              -0.019701 -0.031341  0.027758 -0.116761  0.052492   \n",
       "gastronomisch         -0.027528 -0.063387  0.014021 -0.078177  0.059397   \n",
       "mentionner            -0.096231 -0.035907 -0.007050 -0.072055  0.002236   \n",
       "tlayuda                0.147453 -0.043849  0.185900 -0.135442  0.066431   \n",
       "12.98                  0.126674 -0.008861  0.027590  0.171539  0.178407   \n",
       "1.8                    0.015904 -0.099715  0.015268  0.023165  0.065181   \n",
       "la'wan                 0.054991 -0.171233 -0.030778 -0.079318  0.098345   \n",
       "copoli                -0.013626 -0.079290  0.127256 -0.026271 -0.009722   \n",
       "unfiltered_tap        -0.028054  0.158814 -0.057763  0.075107 -0.047897   \n",
       "das_lokal_liegt       -0.067263 -0.118159  0.007359 -0.106028  0.102627   \n",
       "shredded_iceberg       0.044758  0.108907  0.018254  0.114377  0.171440   \n",
       "panty                  0.058988  0.066161 -0.070886 -0.117642  0.107217   \n",
       "racial_profiling      -0.047535 -0.021964 -0.057480 -0.019847  0.068189   \n",
       "softie                 0.094245  0.105707  0.022846 -0.071944  0.027392   \n",
       "2-bite                 0.140784  0.257919  0.013713  0.111766 -0.091504   \n",
       "stancatos             -0.057148 -0.006942 -0.092197 -0.121852  0.007051   \n",
       "si_c'était            -0.107894  0.000574  0.029121 -0.129268 -0.050664   \n",
       "rustikale_einrichtung -0.031700 -0.056725 -0.016485 -0.142037  0.031182   \n",
       "roger_clyne           -0.136911 -0.143118 -0.028211  0.038061  0.142315   \n",
       "buerre                 0.087922  0.030625  0.058320  0.140025  0.000065   \n",
       "16_oz_t_bone           0.150370 -0.059488  0.087915  0.211372  0.009845   \n",
       "il_faudra             -0.022638 -0.016636 -0.025979 -0.101953 -0.005451   \n",
       "homefry                0.116570  0.282813  0.126190  0.150699  0.032790   \n",
       "rum_runners           -0.094877 -0.134352 -0.016583 -0.050102  0.043453   \n",
       "brust                  0.092016 -0.072978  0.014328 -0.076084  0.027863   \n",
       "würzen                 0.000163 -0.071786  0.006470 -0.116675  0.053546   \n",
       "st_martin              0.008220 -0.105143  0.053471 -0.125264  0.013906   \n",
       "verweilen_ein         -0.053924 -0.063965  0.001616 -0.125484  0.077436   \n",
       "\n",
       "                         ...           90        91        92        93  \\\n",
       "-PRON-                   ...    -0.080210 -0.049757  0.025534  0.015274   \n",
       "be                       ...    -0.169080 -0.003657 -0.147224 -0.002735   \n",
       "the                      ...    -0.023712 -0.109440 -0.081754  0.105876   \n",
       "and                      ...    -0.030788 -0.053569 -0.044293  0.042094   \n",
       "a                        ...    -0.041942  0.023825 -0.163675  0.137406   \n",
       "to                       ...    -0.111642 -0.035838  0.109139  0.119915   \n",
       "have                     ...    -0.177000  0.035325 -0.133072  0.059747   \n",
       "of                       ...    -0.088307  0.018960  0.040729  0.075636   \n",
       "not                      ...    -0.033276 -0.031341  0.009931  0.066965   \n",
       "for                      ...    -0.053567 -0.012096 -0.098778  0.110779   \n",
       "in                       ...     0.016027  0.138046  0.131412 -0.012072   \n",
       "with                     ...    -0.024873  0.028910  0.006591  0.174381   \n",
       "that                     ...    -0.047086 -0.030434  0.006797  0.081248   \n",
       "but                      ...    -0.036399  0.061405 -0.123044  0.048308   \n",
       "good                     ...    -0.172629 -0.116435 -0.127564  0.069711   \n",
       "this                     ...    -0.109729 -0.058495 -0.119857  0.192714   \n",
       "food                     ...    -0.115787 -0.063886 -0.235172  0.043143   \n",
       "on                       ...    -0.011821  0.054055  0.091993  0.097342   \n",
       "do                       ...    -0.041785  0.160770 -0.031583  0.035659   \n",
       "place                    ...    -0.201310  0.044618 -0.249970  0.160110   \n",
       "so                       ...    -0.045152 -0.030313 -0.031205  0.001910   \n",
       "get                      ...    -0.190656  0.016084 -0.114823  0.007541   \n",
       "go                       ...    -0.030755  0.031253 -0.056464 -0.028093   \n",
       "at                       ...    -0.017208 -0.006592 -0.071516  0.113403   \n",
       "order                    ...    -0.144986  0.040197  0.008802 -0.073201   \n",
       "as                       ...    -0.107823  0.066603 -0.099904  0.107366   \n",
       "great                    ...    -0.158378 -0.088051 -0.200404  0.098581   \n",
       "very                     ...    -0.139809 -0.074991 -0.125387  0.037095   \n",
       "there                    ...     0.021424  0.094851 -0.153519  0.005353   \n",
       "like                     ...    -0.080010  0.170374 -0.128055  0.247395   \n",
       "...                      ...          ...       ...       ...       ...   \n",
       "active_duty_military     ...    -0.083939 -0.010948 -0.158681  0.009105   \n",
       "santana_row              ...    -0.088767 -0.028912 -0.079247  0.021594   \n",
       "seltsame                 ...    -0.036584 -0.085108  0.080140 -0.126538   \n",
       "gastronomisch            ...    -0.044552 -0.057372  0.051270 -0.161742   \n",
       "mentionner               ...    -0.070042 -0.121707 -0.025584 -0.093653   \n",
       "tlayuda                  ...    -0.112881  0.022137  0.038144  0.109481   \n",
       "12.98                    ...     0.018818  0.018999  0.004139  0.079557   \n",
       "1.8                      ...    -0.066356 -0.022689 -0.046109 -0.042808   \n",
       "la'wan                   ...    -0.151256 -0.057754 -0.123623  0.113851   \n",
       "copoli                   ...    -0.161245 -0.139925 -0.055061  0.081608   \n",
       "unfiltered_tap           ...    -0.153777  0.040516 -0.059231  0.018697   \n",
       "das_lokal_liegt          ...    -0.005626 -0.007377  0.006666 -0.107501   \n",
       "shredded_iceberg         ...     0.086990 -0.114200  0.109720  0.071394   \n",
       "panty                    ...    -0.132313 -0.092307 -0.247743 -0.057379   \n",
       "racial_profiling         ...    -0.025000  0.028655 -0.160886  0.065731   \n",
       "softie                   ...    -0.094743 -0.072430 -0.021318  0.132094   \n",
       "2-bite                   ...    -0.117606  0.017328 -0.014158  0.009548   \n",
       "stancatos                ...    -0.122582  0.006995 -0.185685  0.073217   \n",
       "si_c'était               ...    -0.051624 -0.094695 -0.050911 -0.107928   \n",
       "rustikale_einrichtung    ...    -0.009180 -0.062048  0.063408 -0.118905   \n",
       "roger_clyne              ...    -0.208557 -0.220399  0.027990 -0.083523   \n",
       "buerre                   ...     0.080017 -0.042219  0.007026  0.005908   \n",
       "16_oz_t_bone             ...    -0.077491  0.112988 -0.203505 -0.091067   \n",
       "il_faudra                ...    -0.010669 -0.086036 -0.023875 -0.119804   \n",
       "homefry                  ...     0.027425  0.058094 -0.011670  0.128980   \n",
       "rum_runners              ...    -0.262772 -0.029052 -0.054634  0.101642   \n",
       "brust                    ...    -0.016706 -0.043750  0.083264 -0.001440   \n",
       "würzen                   ...     0.028043 -0.046346  0.081733 -0.150837   \n",
       "st_martin                ...    -0.120109 -0.048794 -0.115922  0.065895   \n",
       "verweilen_ein            ...    -0.029124 -0.046472  0.044894 -0.127964   \n",
       "\n",
       "                             94        95        96        97        98  \\\n",
       "-PRON-                -0.086829 -0.124214  0.064420  0.040135 -0.064753   \n",
       "be                    -0.115380 -0.210662  0.005098  0.041456  0.005519   \n",
       "the                   -0.073468 -0.165876  0.151434  0.016965 -0.156994   \n",
       "and                   -0.086180 -0.017890  0.075092 -0.055175 -0.031652   \n",
       "a                     -0.065831 -0.101458  0.000391 -0.006488 -0.038800   \n",
       "to                    -0.073793 -0.023686 -0.009697 -0.005111 -0.104554   \n",
       "have                  -0.014941  0.053005  0.155703 -0.023206 -0.083596   \n",
       "of                    -0.002357 -0.060602  0.052249 -0.035362 -0.188302   \n",
       "not                   -0.087295 -0.164688  0.060534  0.188567  0.038080   \n",
       "for                   -0.126084 -0.069208  0.050522  0.049058 -0.100268   \n",
       "in                    -0.124181 -0.074119  0.112797 -0.041809 -0.079513   \n",
       "with                  -0.071311  0.014584 -0.084852 -0.033206 -0.079724   \n",
       "that                   0.004661 -0.167754  0.175307  0.010700 -0.183197   \n",
       "but                   -0.147421 -0.051132  0.143027  0.057177 -0.129424   \n",
       "good                  -0.103830 -0.079846  0.009578 -0.009213 -0.078922   \n",
       "this                  -0.046124 -0.225118  0.153466 -0.044794 -0.088931   \n",
       "food                  -0.092227 -0.117109  0.159203 -0.006079  0.003678   \n",
       "on                    -0.024247  0.000250 -0.083258  0.177199 -0.079912   \n",
       "do                     0.102493 -0.005057 -0.007104  0.096568 -0.125769   \n",
       "place                  0.035160 -0.078361  0.048521  0.050932  0.008414   \n",
       "so                    -0.068078 -0.054393  0.063075 -0.141343 -0.056523   \n",
       "get                   -0.193399 -0.064437  0.047417  0.009790 -0.143821   \n",
       "go                     0.015045 -0.109407  0.073646 -0.148390 -0.078517   \n",
       "at                    -0.062966 -0.055145  0.084811  0.010498 -0.177061   \n",
       "order                 -0.161401 -0.006538  0.008493  0.062066 -0.115829   \n",
       "as                    -0.028048 -0.078120  0.069437 -0.169679 -0.031027   \n",
       "great                 -0.097083  0.058702  0.023418 -0.016989  0.032076   \n",
       "very                  -0.118574 -0.091542  0.025768  0.005672 -0.054532   \n",
       "there                 -0.109259  0.065827  0.056533 -0.031071 -0.125726   \n",
       "like                  -0.112259 -0.018157 -0.013934  0.059989 -0.122854   \n",
       "...                         ...       ...       ...       ...       ...   \n",
       "active_duty_military   0.071534  0.081744  0.050954  0.092710 -0.011673   \n",
       "santana_row           -0.027070 -0.080100  0.132137 -0.079408 -0.118926   \n",
       "seltsame              -0.095934  0.040602 -0.004856  0.034584 -0.093786   \n",
       "gastronomisch         -0.096971 -0.044673 -0.003418  0.028656 -0.108050   \n",
       "mentionner            -0.026203  0.051164 -0.024464  0.083263 -0.020001   \n",
       "tlayuda               -0.052076 -0.028290 -0.114759 -0.049156 -0.055461   \n",
       "12.98                  0.075803  0.028352  0.014360  0.157180  0.025728   \n",
       "1.8                    0.064343 -0.037488 -0.100119  0.051975 -0.029452   \n",
       "la'wan                 0.071573 -0.139447 -0.022044 -0.161350 -0.062607   \n",
       "copoli                -0.024436 -0.014512 -0.005673 -0.052711 -0.073393   \n",
       "unfiltered_tap         0.096502 -0.118470  0.078701  0.165676  0.113941   \n",
       "das_lokal_liegt       -0.077264  0.015495  0.015058  0.035925 -0.111112   \n",
       "shredded_iceberg      -0.088573 -0.082660 -0.112285  0.028193 -0.213291   \n",
       "panty                 -0.018347 -0.111343  0.067737  0.167651  0.128654   \n",
       "racial_profiling       0.045111 -0.050579  0.132683  0.034007 -0.045011   \n",
       "softie                -0.027889  0.121042 -0.040436  0.132192  0.026054   \n",
       "2-bite                -0.104912  0.035710  0.020182  0.111243 -0.090909   \n",
       "stancatos              0.038770 -0.002868  0.116158 -0.005037  0.102094   \n",
       "si_c'était            -0.019283  0.007699 -0.007755 -0.008725 -0.049221   \n",
       "rustikale_einrichtung -0.092025  0.004460 -0.051941  0.035309 -0.106882   \n",
       "roger_clyne           -0.018991 -0.051335  0.049166  0.052005  0.080303   \n",
       "buerre                -0.062098 -0.006033 -0.100040  0.101852 -0.035691   \n",
       "16_oz_t_bone           0.019508 -0.065058 -0.096854  0.013286 -0.101238   \n",
       "il_faudra             -0.053642  0.053871 -0.045014  0.077664 -0.003368   \n",
       "homefry               -0.020525 -0.117377  0.136066  0.007245 -0.114365   \n",
       "rum_runners           -0.012324 -0.134599  0.142007  0.020505  0.034816   \n",
       "brust                 -0.034947  0.045480  0.031839 -0.042802 -0.066962   \n",
       "würzen                -0.100034  0.015725 -0.059749  0.053818 -0.087188   \n",
       "st_martin              0.045484 -0.049816 -0.017363  0.001265 -0.092770   \n",
       "verweilen_ein         -0.088891 -0.000579  0.011269  0.065944 -0.083186   \n",
       "\n",
       "                             99  \n",
       "-PRON-                 0.124752  \n",
       "be                    -0.022719  \n",
       "the                    0.009750  \n",
       "and                    0.033878  \n",
       "a                      0.170717  \n",
       "to                     0.023787  \n",
       "have                  -0.067215  \n",
       "of                     0.044007  \n",
       "not                    0.041429  \n",
       "for                    0.079133  \n",
       "in                    -0.071991  \n",
       "with                  -0.041860  \n",
       "that                  -0.089717  \n",
       "but                   -0.020618  \n",
       "good                   0.056002  \n",
       "this                  -0.025346  \n",
       "food                  -0.005671  \n",
       "on                     0.052545  \n",
       "do                     0.014746  \n",
       "place                  0.000480  \n",
       "so                     0.014155  \n",
       "get                    0.096789  \n",
       "go                     0.007968  \n",
       "at                    -0.072776  \n",
       "order                  0.089099  \n",
       "as                     0.039416  \n",
       "great                  0.171971  \n",
       "very                   0.126413  \n",
       "there                 -0.216364  \n",
       "like                   0.064167  \n",
       "...                         ...  \n",
       "active_duty_military   0.044877  \n",
       "santana_row            0.063933  \n",
       "seltsame               0.145502  \n",
       "gastronomisch          0.188143  \n",
       "mentionner             0.170186  \n",
       "tlayuda                0.068606  \n",
       "12.98                  0.251373  \n",
       "1.8                    0.256021  \n",
       "la'wan                -0.052625  \n",
       "copoli                 0.130494  \n",
       "unfiltered_tap         0.112050  \n",
       "das_lokal_liegt        0.197808  \n",
       "shredded_iceberg      -0.086563  \n",
       "panty                  0.056562  \n",
       "racial_profiling      -0.190862  \n",
       "softie                 0.061769  \n",
       "2-bite                 0.105992  \n",
       "stancatos              0.001405  \n",
       "si_c'était             0.166984  \n",
       "rustikale_einrichtung  0.173018  \n",
       "roger_clyne           -0.032558  \n",
       "buerre                 0.004768  \n",
       "16_oz_t_bone           0.305150  \n",
       "il_faudra              0.183753  \n",
       "homefry                0.105661  \n",
       "rum_runners           -0.129387  \n",
       "brust                  0.128032  \n",
       "würzen                 0.158437  \n",
       "st_martin              0.100528  \n",
       "verweilen_ein          0.149842  \n",
       "\n",
       "[80588 rows x 100 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in food2vec.wv.vocab.iteritems()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.wv.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80588, 100)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has 80,588 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. This is the vector representation for each term.\n",
    "\n",
    "Next, we will save this as a gensim model and it can be reused in another notebook for converting any text into the 100-dimensional vector representation. \n",
    "\n",
    "This will provide more _meaning_ to a review, compared to the LDA topic modeling. The word2vec can provide information about what the text is about and if the words have a positive-negative context. The LDA only provides information about the topics (sometimes hard to interpret), but very little information about the positive/negative sentiment of the review. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
